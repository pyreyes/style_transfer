{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import editdistance\n",
    "\n",
    "from .data import data as data\n",
    "from .cuda import CUDA\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# BLEU functions\n",
    "#    (ran some comparisons and it matches moses's multi-bleu.perl)\n",
    "def bleu_stats(hypothesis, reference):\n",
    "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in range(1, 5):\n",
    "        s_ngrams = Counter(\n",
    "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
    "        )\n",
    "        r_ngrams = Counter(\n",
    "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
    "        )\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
    "    return stats\n",
    "\n",
    "def bleu(stats):\n",
    "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
    "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum(\n",
    "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
    "    ) / 4.\n",
    "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def get_edit_distance(hypotheses, reference):\n",
    "    ed = 0\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        ed += editdistance.eval(hyp, ref)\n",
    "\n",
    "    return ed * 1.0 / len(hypotheses)\n",
    "\n",
    "\n",
    "def decode_minibatch(max_len, start_id, model, src_input, srclens, srcmask,\n",
    "        aux_input, auxlens, auxmask):\n",
    "    \"\"\" argmax decoding \"\"\"\n",
    "    # Initialize target with <s> for every sentence\n",
    "    tgt_input = Variable(torch.LongTensor(\n",
    "        [\n",
    "            [start_id] for i in range(src_input.size(0))\n",
    "        ]\n",
    "    ))\n",
    "    if CUDA:\n",
    "        tgt_input = tgt_input.cuda()\n",
    "\n",
    "    for i in range(max_len):\n",
    "        # run input through the model\n",
    "        decoder_logit, word_probs = model(src_input, tgt_input, srcmask, srclens,\n",
    "            aux_input, auxmask, auxlens)\n",
    "        decoder_argmax = word_probs.data.cpu().numpy().argmax(axis=-1)\n",
    "        # select the predicted \"next\" tokens, attach to target-side inputs\n",
    "        next_preds = Variable(torch.from_numpy(decoder_argmax[:, -1]))\n",
    "        if CUDA:\n",
    "            next_preds = next_preds.cuda()\n",
    "        tgt_input = torch.cat((tgt_input, next_preds.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return tgt_input\n",
    "\n",
    "def decode_dataset(model, src, tgt, config):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    inputs = []\n",
    "    preds = []\n",
    "    auxs = []\n",
    "    ground_truths = []\n",
    "    for j in range(0, len(src['data']), config['data']['batch_size']):\n",
    "        sys.stdout.write(\"\\r%s/%s...\" % (j, len(src['data'])))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # get batch\n",
    "        input_content, input_aux, output = data.minibatch(\n",
    "            src, tgt, j, \n",
    "            config['data']['batch_size'], \n",
    "            config['data']['max_len'], \n",
    "            config['model']['model_type'],\n",
    "            is_test=True)\n",
    "        input_lines_src, output_lines_src, srclens, srcmask, indices = input_content\n",
    "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
    "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
    "\n",
    "        # TODO -- beam search\n",
    "        tgt_pred = decode_minibatch(\n",
    "            config['data']['max_len'], tgt['tok2id']['<s>'], \n",
    "            model, input_lines_src, srclens, srcmask,\n",
    "            input_ids_aux, auxlens, auxmask)\n",
    "\n",
    "        # convert seqs to tokens\n",
    "        def ids_to_toks(tok_seqs, id2tok):\n",
    "            out = []\n",
    "            # take off the gpu\n",
    "            tok_seqs = tok_seqs.cpu().numpy()\n",
    "            # convert to toks, cut off at </s>, delete any start tokens (preds were kickstarted w them)\n",
    "            for line in tok_seqs:\n",
    "                toks = [id2tok[x] for x in line]\n",
    "                if '<s>' in toks: \n",
    "                    toks.remove('<s>')\n",
    "                cut_idx = toks.index('</s>') if '</s>' in toks else len(toks)\n",
    "                out.append( toks[:cut_idx] )\n",
    "            # unsort\n",
    "            out = data.unsort(out, indices)\n",
    "            return out\n",
    "\n",
    "        # convert inputs/preds/targets/aux to human-readable form\n",
    "        inputs += ids_to_toks(output_lines_src, src['id2tok'])\n",
    "        preds += ids_to_toks(tgt_pred, tgt['id2tok'])\n",
    "        ground_truths += ids_to_toks(output_lines_tgt, tgt['id2tok'])\n",
    "        \n",
    "        if config['model']['model_type'] == 'delete':\n",
    "            auxs += [[str(x)] for x in input_ids_aux.data.cpu().numpy()] # because of list comp in inference_metrics()\n",
    "        elif config['model']['model_type'] == 'delete_retrieve':\n",
    "            auxs += ids_to_toks(input_ids_aux, tgt['id2tok'])\n",
    "        elif config['model']['model_type'] == 'seq2seq':\n",
    "            auxs += ['None' for _ in range(len(tgt_pred))]\n",
    "\n",
    "    return inputs, preds, ground_truths, auxs\n",
    "\n",
    "\n",
    "def inference_metrics(model, src, tgt, config):\n",
    "    \"\"\" decode and evaluate bleu \"\"\"\n",
    "    inputs, preds, ground_truths, auxs = decode_dataset(\n",
    "        model, src, tgt, config)\n",
    "\n",
    "    bleu = get_bleu(preds, ground_truths)\n",
    "    edit_distance = get_edit_distance(preds, ground_truths)\n",
    "\n",
    "    inputs = [' '.join(seq) for seq in inputs]\n",
    "    preds = [' '.join(seq) for seq in preds]\n",
    "    ground_truths = [' '.join(seq) for seq in ground_truths]\n",
    "    auxs = [' '.join(seq) for seq in auxs]\n",
    "\n",
    "    return bleu, edit_distance, inputs, preds, ground_truths, auxs\n",
    "\n",
    "\n",
    "def evaluate_lpp(model, src, tgt, config):\n",
    "    \"\"\" evaluate log perplexity WITHOUT decoding\n",
    "        (i.e., with teacher forcing)\n",
    "    \"\"\"\n",
    "    weight_mask = torch.ones(len(tgt['tok2id']))\n",
    "    if CUDA:\n",
    "        weight_mask = weight_mask.cuda()\n",
    "    weight_mask[tgt['tok2id']['<pad>']] = 0\n",
    "    loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)\n",
    "    if CUDA:\n",
    "        loss_criterion = loss_criterion.cuda()\n",
    "\n",
    "    losses = []\n",
    "    for j in range(0, len(src['data']), config['data']['batch_size']):\n",
    "        sys.stdout.write(\"\\r%s/%s...\" % (j, len(src['data'])))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # get batch\n",
    "        input_content, input_aux, output = data.minibatch(\n",
    "            src, tgt, j, \n",
    "            config['data']['batch_size'], \n",
    "            config['data']['max_len'], \n",
    "            config['model']['model_type'],\n",
    "            is_test=True)\n",
    "        input_lines_src, _, srclens, srcmask, _ = input_content\n",
    "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
    "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
    "\n",
    "        decoder_logit, decoder_probs = model(\n",
    "            input_lines_src, input_lines_tgt, srcmask, srclens,\n",
    "            input_ids_aux, auxlens, auxmask)\n",
    "\n",
    "        loss = loss_criterion(\n",
    "            decoder_logit.contiguous().view(-1, len(tgt['tok2id'])),\n",
    "            output_lines_tgt.view(-1)\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
